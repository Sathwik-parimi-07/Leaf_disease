{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "eafb61d7",
      "metadata": {
        "papermill": {
          "duration": 0.00303,
          "end_time": "2025-05-15T16:13:33.426835",
          "exception": false,
          "start_time": "2025-05-15T16:13:33.423805",
          "status": "completed"
        },
        "tags": [],
        "id": "eafb61d7"
      },
      "source": [
        "# PyTorch Leaf Disease Classification Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a416f1ea",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-15T16:13:33.432442Z",
          "iopub.status.busy": "2025-05-15T16:13:33.432209Z",
          "iopub.status.idle": "2025-05-15T16:14:54.432163Z",
          "shell.execute_reply": "2025-05-15T16:14:54.431170Z"
        },
        "papermill": {
          "duration": 81.004397,
          "end_time": "2025-05-15T16:14:54.433735",
          "exception": false,
          "start_time": "2025-05-15T16:13:33.429338",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a416f1ea",
        "outputId": "2db70261-c45c-4b97-c78d-ae1eeaafea42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Cell 1: Install PyTorch\n",
        "!pip install -q torch torchvision torchaudio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "8d645003",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-15T16:14:54.476947Z",
          "iopub.status.busy": "2025-05-15T16:14:54.476694Z",
          "iopub.status.idle": "2025-05-15T16:14:57.879966Z",
          "shell.execute_reply": "2025-05-15T16:14:57.879176Z"
        },
        "papermill": {
          "duration": 3.426646,
          "end_time": "2025-05-15T16:14:57.881418",
          "exception": false,
          "start_time": "2025-05-15T16:14:54.454772",
          "status": "completed"
        },
        "tags": [],
        "id": "8d645003"
      },
      "outputs": [],
      "source": [
        "# Cell 2: Imports & Basic Setup\n",
        "from pathlib import Path\n",
        "import os, json, cv2, numpy as np, matplotlib.pyplot as plt\n",
        "import torch\n",
        "from PIL import Image\n",
        "from shapely.geometry import Polygon\n",
        "\n",
        "Real_DIR = Path(r\"D:\\rico\\archive\\annotated_apple_leaf_disease\")\n",
        "Mask_DIR = Path(r\"D:\\rico\\archive\\annotated_apple_leaf_disease\")\n",
        "IMG_SIZE = 224"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "NBgQzayfKRWH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6896c6c5-9fbf-46c2-f28b-14d0fa249e85"
      },
      "id": "NBgQzayfKRWH",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5f8777ed",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-15T16:14:57.923800Z",
          "iopub.status.busy": "2025-05-15T16:14:57.923469Z",
          "iopub.status.idle": "2025-05-15T16:14:57.986140Z",
          "shell.execute_reply": "2025-05-15T16:14:57.985452Z"
        },
        "papermill": {
          "duration": 0.084921,
          "end_time": "2025-05-15T16:14:57.987322",
          "exception": false,
          "start_time": "2025-05-15T16:14:57.902401",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "5f8777ed",
        "outputId": "dccc7433-c360-427a-f6fc-87391dbd3ea7"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'D:\\\\rico\\\\archive\\\\annotated_apple_leaf_disease/dummy_leaf.png'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2-3358634418.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0msample_mask_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWORK_DIR\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"dummy_leaf_mask.png\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdummy_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_img_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdummy_mask\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_mask_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2574\u001b[0m                 \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r+b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2575\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2576\u001b[0;31m                 \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w+b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2577\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2578\u001b[0m             \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:\\\\rico\\\\archive\\\\annotated_apple_leaf_disease/dummy_leaf.png'"
          ]
        }
      ],
      "source": [
        "#Create dummy image & mask :)\n",
        "import numpy as np, cv2\n",
        "from PIL import Image\n",
        "\n",
        "# Dimensions\n",
        "H, W = 224, 224\n",
        "\n",
        "# 1) Dummy leaf image (random noise)\n",
        "dummy_img = (np.random.rand(H, W, 3) * 255).astype(np.uint8)\n",
        "# 2) Dummy mask: circle in center\n",
        "dummy_mask = np.zeros((H, W), dtype=np.uint8)\n",
        "cv2.circle(dummy_mask, (W//2, H//2), 50, 1, -1)\n",
        "\n",
        "# Save to working directory\n",
        "sample_img_path  = mask_DIR / \"dummy_leaf.png\"\n",
        "sample_mask_path = mask_DIR / \"dummy_leaf_mask.png\"\n",
        "\n",
        "Image.fromarray(dummy_img).save(sample_img_path)\n",
        "Image.fromarray(dummy_mask * 255).save(sample_mask_path)\n",
        "\n",
        "print(f\"Dummy image → {sample_img_path}\")\n",
        "print(f\"Dummy mask  → {sample_mask_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "5e195f1e",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-15T16:14:58.030469Z",
          "iopub.status.busy": "2025-05-15T16:14:58.030231Z",
          "iopub.status.idle": "2025-05-15T16:14:58.078031Z",
          "shell.execute_reply": "2025-05-15T16:14:58.077202Z"
        },
        "papermill": {
          "duration": 0.070371,
          "end_time": "2025-05-15T16:14:58.079132",
          "exception": false,
          "start_time": "2025-05-15T16:14:58.008761",
          "status": "completed"
        },
        "tags": [],
        "id": "5e195f1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9685128a-9996-40e5-801b-a6329bf2e317"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Masks generated in D:\\rico\\archive\\annotated_apple_leaf_disease\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from PIL import ImageDraw\n",
        "\n",
        "def json_to_mask(js_path):\n",
        "    with open(js_path) as f:\n",
        "        meta = json.load(f)\n",
        "    h, w = meta['imageHeight'], meta['imageWidth']\n",
        "    mask = Image.new('L', (w, h), 0)\n",
        "    for shp in meta['shapes']:\n",
        "        ImageDraw.Draw(mask).polygon(shp['points'], outline=1, fill=1)\n",
        "    mask = mask.resize((IMG_SIZE, IMG_SIZE), Image.NEAREST)\n",
        "    out_path = WORK_DIR / js_path.relative_to(DATA_DIR).with_suffix('.png').as_posix()\n",
        "    out_path = out_path.replace('/annots/', '/masks/')\n",
        "    Path(out_path).parent.mkdir(parents=True, exist_ok=True)\n",
        "    mask.save(out_path)\n",
        "\n",
        "for split in ['train', 'valid']:\n",
        "    for js in (DATA_DIR/split/'annots').glob('*.json'):\n",
        "        json_to_mask(js)\n",
        "print(\"✓ Masks generated in\", WORK_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "3901c01e",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-15T16:14:58.122759Z",
          "iopub.status.busy": "2025-05-15T16:14:58.122468Z",
          "iopub.status.idle": "2025-05-15T16:14:58.126332Z",
          "shell.execute_reply": "2025-05-15T16:14:58.125798Z"
        },
        "papermill": {
          "duration": 0.027086,
          "end_time": "2025-05-15T16:14:58.127274",
          "exception": false,
          "start_time": "2025-05-15T16:14:58.100188",
          "status": "completed"
        },
        "tags": [],
        "id": "3901c01e"
      },
      "outputs": [],
      "source": [
        "# Cell 3.1: Load & binarize a lesion mask\n",
        "def load_mask(mask_path, size=(224, 224)):\n",
        "    \"\"\"Load mask as 2D binary array.\"\"\"\n",
        "    m = Image.open(mask_path).convert(\"L\").resize(size, Image.NEAREST)\n",
        "    arr = np.array(m)\n",
        "    # Threshold at mid-point to get 0/1\n",
        "    return (arr > 127).astype(np.uint8)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "978858f8",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-15T16:14:58.170817Z",
          "iopub.status.busy": "2025-05-15T16:14:58.170593Z",
          "iopub.status.idle": "2025-05-15T16:15:06.132890Z",
          "shell.execute_reply": "2025-05-15T16:15:06.132078Z"
        },
        "papermill": {
          "duration": 7.985835,
          "end_time": "2025-05-15T16:15:06.134094",
          "exception": false,
          "start_time": "2025-05-15T16:14:58.148259",
          "status": "completed"
        },
        "tags": [],
        "id": "978858f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "outputId": "5824f33a-9b6e-4c82-c995-bdfb81b013f3"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "num_samples should be a positive integer value, but got num_samples=0",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-7-2751407823.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0mnum_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m train_loader = DataLoader(AppleLeafDataset(train_items, transform), batch_size=batch_size,\n\u001b[0m\u001b[1;32m     55\u001b[0m                           shuffle=True, num_workers=num_workers, pin_memory=True)\n\u001b[1;32m     56\u001b[0m val_loader   = DataLoader(AppleLeafDataset(val_items, transform),   batch_size=batch_size,\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device, in_order)\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# map-style\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequentialSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    166\u001b[0m                 \u001b[0;34mf\"num_samples should be a positive integer value, but got num_samples={self.num_samples}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             )\n",
            "\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
          ]
        }
      ],
      "source": [
        "\n",
        "import xml.etree.ElementTree as ET\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "# Collect image paths and labels\n",
        "def collect_split(split: str):\n",
        "    items = []\n",
        "    for xml_fp in (DATA_DIR/split/'annots').glob('*.xml'):\n",
        "        tree = ET.parse(xml_fp)\n",
        "        root = tree.getroot()\n",
        "        img_name = root.find('filename').text\n",
        "        img_path = DATA_DIR/split/'images'/img_name\n",
        "        for obj in root.findall('object'):\n",
        "            lbl = obj.find('name').text\n",
        "            items.append((str(img_path), lbl))\n",
        "    return items\n",
        "\n",
        "train_items = collect_split('train')\n",
        "val_items   = collect_split('valid')\n",
        "\n",
        "# Build class mapping\n",
        "class_names = sorted({lbl for _, lbl in train_items})\n",
        "class_to_idx = {c:i for i,c in enumerate(class_names)}\n",
        "\n",
        "# Transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
        "])\n",
        "\n",
        "# Dataset class\n",
        "class AppleLeafDataset(Dataset):\n",
        "    def __init__(self, items, transform=None):\n",
        "        self.items = items\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.items)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.items[idx]\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        lbl = class_to_idx[label]\n",
        "        return img, lbl\n",
        "\n",
        "# DataLoaders _______.>\n",
        "batch_size = 32\n",
        "num_workers = min(4, os.cpu_count())\n",
        "\n",
        "train_loader = DataLoader(AppleLeafDataset(train_items, transform), batch_size=batch_size,\n",
        "                          shuffle=True, num_workers=num_workers, pin_memory=True)\n",
        "val_loader   = DataLoader(AppleLeafDataset(val_items, transform),   batch_size=batch_size,\n",
        "                          shuffle=False,num_workers=num_workers,pin_memory=True)\n",
        "\n",
        "print(f\"✔ Classes: {class_names}\")\n",
        "print(f\"Dataloaders ready: train batches = {len(train_loader)}, valid = {len(val_loader)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "3967f3fa",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-15T16:15:06.177555Z",
          "iopub.status.busy": "2025-05-15T16:15:06.177060Z",
          "iopub.status.idle": "2025-05-15T16:16:27.583082Z",
          "shell.execute_reply": "2025-05-15T16:16:27.582036Z"
        },
        "papermill": {
          "duration": 81.429044,
          "end_time": "2025-05-15T16:16:27.584417",
          "exception": false,
          "start_time": "2025-05-15T16:15:06.155373",
          "status": "completed"
        },
        "tags": [],
        "id": "3967f3fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "outputId": "cb24ab25-ec39-417f-fde7-a09821939697"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 141MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/init.py:511: UserWarning: Initializing zero-element tensors is a no-op\n",
            "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_loader' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-8-2276477385.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
          ]
        }
      ],
      "source": [
        "#  (Transfer Learning since no time ) & Training Loop\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Base model\n",
        "model = models.resnet50(pretrained=True)\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, len(class_names))\n",
        "model = model.to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.fc.parameters(), lr=1e-3)\n",
        "\n",
        "# Training loop (warm-up)\n",
        "best_acc = 0.0\n",
        "for epoch in range(5):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = total = 0\n",
        "    for imgs, labels in train_loader:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(imgs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * imgs.size(0)\n",
        "        _, preds = outputs.max(1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "    train_loss = running_loss / total\n",
        "    train_acc = correct / total\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = val_corr = val_tot = 0\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in val_loader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            out = model(imgs)\n",
        "            l = criterion(out, labels)\n",
        "            val_loss += l.item() * imgs.size(0)\n",
        "            _, p = out.max(1)\n",
        "            val_corr += (p == labels).sum().item()\n",
        "            val_tot  += labels.size(0)\n",
        "    val_loss /= val_tot\n",
        "    val_acc = val_corr / val_tot\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/5 — \"\n",
        "          f\"train loss {train_loss:.4f}, acc {train_acc:.4f} | \"\n",
        "          f\"val loss {val_loss:.4f}, acc {val_acc:.4f}\")\n",
        "\n",
        "    if val_acc > best_acc:\n",
        "        best_acc = val_acc\n",
        "        torch.save(model.state_dict(), 'best_resnet50.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "9a45953f",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-15T16:16:27.674187Z",
          "iopub.status.busy": "2025-05-15T16:16:27.673899Z",
          "iopub.status.idle": "2025-05-15T16:26:25.035398Z",
          "shell.execute_reply": "2025-05-15T16:26:25.034419Z"
        },
        "papermill": {
          "duration": 597.41171,
          "end_time": "2025-05-15T16:26:25.062767",
          "exception": false,
          "start_time": "2025-05-15T16:16:27.651057",
          "status": "completed"
        },
        "tags": [],
        "id": "9a45953f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "3283b611-7cc3-48dc-ecc0-592e6e6b79eb"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_loader' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-9-70808876.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0moptimizer_ft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
          ]
        }
      ],
      "source": [
        "#  tuning  Network\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "optimizer_ft = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "for epoch in range(15):\n",
        "    model.train()\n",
        "    running_loss = correct = total = 0\n",
        "    for imgs, labels in train_loader:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        optimizer_ft.zero_grad()\n",
        "        outputs = model(imgs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer_ft.step()\n",
        "        running_loss += loss.item() * imgs.size(0)\n",
        "        _, preds = outputs.max(1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "    train_loss = running_loss / total\n",
        "    train_acc = correct / total\n",
        "\n",
        "    # Validation same as above\n",
        "    model.eval()\n",
        "    val_loss = val_corr = val_tot = 0\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in val_loader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            out = model(imgs)\n",
        "            l = criterion(out, labels)\n",
        "            val_loss += l.item() * imgs.size(0)\n",
        "            _, p = out.max(1)\n",
        "            val_corr += (p == labels).sum().item()\n",
        "            val_tot  += labels.size(0)\n",
        "    val_loss /= val_tot\n",
        "    val_acc = val_corr / val_tot\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/15 — \"\n",
        "          f\"train loss {train_loss:.4f}, acc {train_acc:.4f} | \"\n",
        "          f\"val loss {val_loss:.4f}, acc {val_acc:.4f}\")\n",
        "\n",
        "torch.save(model.state_dict(), 'leaf_classifier_final.pt')\n",
        "print(\" Model saved to leaf_classifier_final.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "e27214d5",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-15T16:26:25.120010Z",
          "iopub.status.busy": "2025-05-15T16:26:25.119727Z",
          "iopub.status.idle": "2025-05-15T16:26:26.132978Z",
          "shell.execute_reply": "2025-05-15T16:26:26.132103Z"
        },
        "papermill": {
          "duration": 1.043825,
          "end_time": "2025-05-15T16:26:26.134629",
          "exception": false,
          "start_time": "2025-05-15T16:26:25.090804",
          "status": "completed"
        },
        "tags": [],
        "id": "e27214d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "outputId": "0a9e0ee3-a915-4d12-bed8-d04656f8b931"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'best_resnet50.pt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-10-2610744312.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Load best model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'best_resnet50.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1423\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1425\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1426\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1427\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    749\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"w\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'best_resnet50.pt'"
          ]
        }
      ],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "# Load  model\n",
        "model.load_state_dict(torch.load('best_resnet50.pt'))\n",
        "model.eval()\n",
        "\n",
        "features = gradients = None\n",
        "def forward_hook(module, inp, out):\n",
        "    global features\n",
        "    features = out.detach()\n",
        "\n",
        "def backward_hook(module, grad_in, grad_out):\n",
        "    global gradients\n",
        "    gradients = grad_out[0].detach()\n",
        "\n",
        "target_layer = model.layer4[-1].conv3\n",
        "target_layer.register_forward_hook(forward_hook)\n",
        "target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "def make_gradcam_heatmap(img_tensor, thresh=0.5):\n",
        "    model.zero_grad()\n",
        "    preds = model(img_tensor)\n",
        "    class_idx = preds.argmax(dim=1).item()\n",
        "    preds[0, class_idx].backward()\n",
        "    pooled_grads = gradients.mean(dim=[0,2,3])\n",
        "    fmap = features[0]\n",
        "    cam = (pooled_grads[:, None, None] * fmap).sum(dim=0).cpu().numpy()\n",
        "    cam = np.maximum(cam, 0)\n",
        "    cam = cam / (cam.max() + 1e-9)\n",
        "    mask = (cam > thresh).astype(np.uint8) * 255\n",
        "    return cam, mask\n",
        "\n",
        "# Example visualization\n",
        "sample_img, _ = next(iter(val_loader))\n",
        "img_tensor = sample_img[0].unsqueeze(0).to(device)\n",
        "raw = sample_img[0].permute(1,2,0).cpu().numpy()\n",
        "heat, mask = make_gradcam_heatmap(img_tensor)\n",
        "\n",
        "plt.figure(figsize=(8,3))\n",
        "plt.subplot(1,3,1); plt.imshow(raw); plt.title('Leaf'); plt.axis('off')\n",
        "plt.subplot(1,3,2); plt.imshow(mask, cmap='gray'); plt.title('Mask'); plt.axis('off')\n",
        "plt.subplot(1,3,3); plt.imshow(heat, cmap='jet'); plt.title('Grad-CAM'); plt.axis('off')\n",
        "plt.tight_layout(); plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "b5d6d99e",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-15T16:26:26.184235Z",
          "iopub.status.busy": "2025-05-15T16:26:26.183725Z",
          "iopub.status.idle": "2025-05-15T16:26:26.243155Z",
          "shell.execute_reply": "2025-05-15T16:26:26.242238Z"
        },
        "papermill": {
          "duration": 0.085252,
          "end_time": "2025-05-15T16:26:26.244263",
          "exception": true,
          "start_time": "2025-05-15T16:26:26.159011",
          "status": "failed"
        },
        "tags": [],
        "id": "b5d6d99e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "outputId": "4fef722c-af90-47e2-9cb7-73bd065939db"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'D:\\\\rico\\\\archive\\\\annotated_apple_leaf_disease/dummy_leaf.png'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-11-3030059542.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Example on our dummy data:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mvalidate_focus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_img_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_mask_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-11-3030059542.py\u001b[0m in \u001b[0;36mvalidate_focus\u001b[0;34m(img_path, mask_path, model, thresh)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mvalidate_focus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# 1) Load & preprocess image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mpil\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpil\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mraw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpil\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3504\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3505\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3506\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3507\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:\\\\rico\\\\archive\\\\annotated_apple_leaf_disease/dummy_leaf.png'"
          ]
        }
      ],
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def validate_focus(img_path, mask_path, model, thresh=0.5):\n",
        "    # 1) Load & preprocess image\n",
        "    pil = Image.open(img_path).convert(\"RGB\")\n",
        "    inp = transform(pil).unsqueeze(0).to(device)\n",
        "    raw = np.array(pil)\n",
        "\n",
        "    # 2) Grad-CAM + binarize\n",
        "    cam, bin_mask = make_gradcam_heatmap(inp, thresh=thresh)\n",
        "\n",
        "    # 3) Ground-truth mask\n",
        "    gt = load_mask(mask_path)\n",
        "\n",
        "    # 4) Compute IoU\n",
        "    iou = binary_iou(bin_mask, gt)\n",
        "    print(f\"IoU (Grad-CAM vs. GT): {iou:.4f}\")\n",
        "\n",
        "    # 5) Plot 4-panel figure\n",
        "    fig, axs = plt.subplots(1, 4, figsize=(16,4))\n",
        "    axs[0].imshow(raw);            axs[0].set_title(\"Original\");           axs[0].axis(\"off\")\n",
        "    axs[1].imshow(gt, cmap=\"gray\");axs[1].set_title(\"GT Lesion Mask\");    axs[1].axis(\"off\")\n",
        "    axs[2].imshow(raw); axs[2].imshow(cam, cmap=\"jet\", alpha=0.5);             axs[2].set_title(\"Grad-CAM Overlay\");  axs[2].axis(\"off\")\n",
        "    axs[3].imshow(bin_mask, cmap=\"gray\"); axs[3].set_title(f\"Binarized (Th={thresh})\\nIoU={iou:.2f}\");    axs[3].axis(\"off\")\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "# Example on our dummy data:\n",
        "validate_focus(sample_img_path, sample_mask_path, model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1d515ec",
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "id": "e1d515ec"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import pandas as pd\n",
        "\n",
        "model.eval()\n",
        "all_preds, all_labels = [], []\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in val_loader:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        outputs = model(imgs)\n",
        "        _, preds = outputs.max(1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Create report & confusion matrix\n",
        "report_dict = classification_report(all_labels, all_preds,\n",
        "                                    target_names=class_names,\n",
        "                                    output_dict=True)\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "# Convert to DataFrames\n",
        "df_report = pd.DataFrame(report_dict).transpose()\n",
        "df_cm     = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
        "\n",
        "\n",
        "df_report.to_csv('classification_report.csv', index=True)\n",
        "df_cm.to_csv('confusion_matrix.csv', index=True)\n",
        "\n",
        "# Display\n",
        "print(\"=== Classification Report ===\")\n",
        "display(df_report.style.format(\"{:.4f}\"))\n",
        "print(\"\\n=== Confusion Matrix ===\")\n",
        "display(df_cm)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e1f5778",
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "id": "6e1f5778"
      },
      "outputs": [],
      "source": [
        "\n",
        "import zipfile\n",
        "\n",
        "files_to_zip = [\n",
        "    'classification_report.csv',\n",
        "    'confusion_matrix.csv',\n",
        "    'best_resnet50.pt',            # your saved best model\n",
        "    'leaf_classifier_final.pt'     # final fine-tuned model , omg sathwik!\n",
        "]\n",
        "\n",
        "with zipfile.ZipFile('results_bundle.zip', 'w', zipfile.ZIP_DEFLATED) as zf:\n",
        "    for fn in files_to_zip:\n",
        "        if os.path.exists(fn):\n",
        "            zf.write(fn)\n",
        "        else:\n",
        "            print(f\"File not found, skipping: {fn}\")\n",
        "\n",
        "print(\"Packed files into results_bundle.zip\")\n"
      ]
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 3079341,
          "sourceId": 5295854,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 31011,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 778.35195,
      "end_time": "2025-05-15T16:26:27.691891",
      "environment_variables": {},
      "exception": true,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2025-05-15T16:13:29.339941",
      "version": "2.6.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}